{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32475d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_unet(nn.Module):\n",
    "    def __init__(self,in_channels,out__channels): #the size (batch,in_channels,54,81)\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(in_channels, 21, kernel_size=4, stride=2, padding=1) \n",
    "        self.enc_conv2 = nn.Conv2d(21, 32, kernel_size=4, stride=2, padding=1)  \n",
    "        self.enc_conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc_conv4 = nn.Conv2d(64, 96, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(96*3*5, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2_mu = nn.Linear(128, in_channels)\n",
    "        self.fc2_logvar = nn.Linear(128, in_channels)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(in_channels, 128)\n",
    "        self.fc4 = nn.Linear(128, 96*3*5)\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(96*2, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(64*2, 32, kernel_size=4, stride=2, padding=1)  \n",
    "        self.dec_conv3 = nn.ConvTranspose2d(32*2, 21, kernel_size=4, stride=2, padding=1)  \n",
    "        self.dec_conv4 = nn.ConvTranspose2d(21*2, 3, kernel_size=4, stride=2, padding=1)   \n",
    "        self.last_conv= nn.ConvTranspose2d(20,out__channels , kernel_size=3, stride=1, padding=1)   # with the channel of the input\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.enc_conv1(x))\n",
    "        h2 = torch.relu(self.enc_conv2(h1))\n",
    "        h3 = torch.relu(self.enc_conv3(h2))\n",
    "        h4 = torch.relu(self.enc_conv4(h3))\n",
    "        h = h4.view(-1,  96*3*5)\n",
    "        h = torch.relu(self.fc1(h))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return self.fc2_mu(h), self.fc2_logvar(h) ,[h4,h3,h2,h1]\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z,in_x,layers):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        h = torch.relu(self.fc4(h))\n",
    "        h = h.view(-1, 96, 3, 5)\n",
    "        h=torch.cat((h,layers[0]),dim=1)\n",
    "        h = torch.relu(self.dec_conv1(h))\n",
    "        h=torch.cat((h,layers[1]),dim=1)\n",
    "        h = torch.relu(self.dec_conv2(h))\n",
    "        padding = (0,0, (13-h.shape[2])//2,(13-h.shape[2]) - (13-h.shape[2]) // 2)\n",
    "        padded_h = F.pad(h, padding)      \n",
    "        h=torch.cat((padded_h,layers[2]),dim=1)\n",
    "        h = torch.relu(self.dec_conv3(h))\n",
    "        padding = (0,0, (27-h.shape[2])//2,(27-h.shape[2]) - (27-h.shape[2]) // 2)\n",
    "        padded_h = F.pad(h, padding)\n",
    "        h=torch.cat((padded_h,layers[3]),dim=1)\n",
    "        h = self.dec_conv4(h)\n",
    "        padding = (0, 81-h.shape[3], (54-h.shape[2])//2,(54-h.shape[2]) - (54-h.shape[2]) // 2)\n",
    "        padded_h = F.pad(h, padding)\n",
    "        h_cat=torch.cat((padded_h,in_x),dim=1)\n",
    "        h=self.last_conv(h_cat)\n",
    "        return h \n",
    "    \n",
    "    def forward(self, x):\n",
    "        in_x=x.clone()\n",
    "        mu, logvar,layers= self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z,in_x,layers), mu, logvar\n",
    "\n",
    "def loss_function_mse(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return MSE + KLD,MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 4))\n",
    "        self.down1 = (Down(4, 8))\n",
    "        self.down2 = (Down(8, 16))\n",
    "        self.down3 = (Down(16, 32))\n",
    "        self.down4 = (Down(32, 64))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down5 = (Down(64, 128 // factor))\n",
    "        self.up1 = (Up(128, 64 // factor, bilinear))\n",
    "        self.up2 = (Up(64, 32 // factor, bilinear))\n",
    "        self.up3 = (Up(32, 16 // factor, bilinear))\n",
    "        self.up4 = (Up(16, 8 // factor, bilinear))\n",
    "        self.up5 = (Up(8, 4, bilinear))\n",
    "        self.outc = (OutConv(n_channels+4, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_x = x\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2)\n",
    "        x = self.up5(x, x1)\n",
    "        in_x=torch.cat([in_x,x],dim=1)\n",
    "        logits = self.outc(in_x)\n",
    "\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
